# Apache Airflow Configuration for MLOps Pipeline

[core]
# The home folder for airflow, default is ~/airflow
airflow_home = /opt/airflow

# The folder where your airflow pipelines live
dags_folder = /opt/airflow/dags

# The folder where airflow should store its log files
base_log_folder = /opt/airflow/logs

# Airflow can store logs remotely in AWS S3, Google Cloud Storage or Elastic Search
remote_logging = False

# The executor class that airflow should use
executor = LocalExecutor

# The SqlAlchemy connection string to the metadata database
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres:5432/airflow

# Whether to load the DAG examples
load_examples = False

# Secret key for saving connection passwords in the db
fernet_key = your_fernet_key_here

# Whether to disable pickling dags
donot_pickle = False

# How long before timing out a python file import
dagbag_import_timeout = 30

# The amount of parallelism as a setting to the executor
parallelism = 32

# The number of task instances allowed to run concurrently by the scheduler
dag_concurrency = 16

# Maximum number of Rendered Task Instance Fields (Template Fields) per task to store in the database
max_num_rendered_ti_fields_per_task = 30

[webserver]
# The base url of your website as airflow cannot guess what domain or
# cname you are using
base_url = http://localhost:8080

# Default timezone to display all dates in the UI
default_ui_timezone = UTC

# The ip specified when starting the web server
web_server_host = 0.0.0.0

# The port on which to run the web server
web_server_port = 8080

# Enable proxy fix for running behind nginx/apache
enable_proxy_fix = False

[scheduler]
# Task instances listen for external kill signal
job_heartbeat_sec = 5

# The scheduler constantly tries to trigger new tasks
scheduler_heartbeat_sec = 5

# Number of seconds after which a DAG file is parsed
min_file_process_interval = 30

# How often to check for DAGs to trigger
dag_dir_list_interval = 300

# Statsd metrics
statsd_on = False

# Interval for checking child processes
scheduler_zombie_task_threshold = 300

# How often to scan for DAGs
dag_dir_list_interval = 300

[operators]
# Default owner for DAGs
default_owner = mlops

# Default retries for operators
default_retries = 1

[smtp]
# SMTP configuration for email alerts
smtp_host = smtp.gmail.com
smtp_starttls = True
smtp_ssl = False
smtp_port = 587
smtp_mail_from = airflow@example.com
