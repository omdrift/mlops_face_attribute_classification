# Guide Apache Airflow pour le Projet MLOps

## üìö Introduction

Ce guide explique comment utiliser Apache Airflow pour orchestrer le pipeline de Machine Learning.

## üèóÔ∏è Architecture Airflow

### Composants

- **Webserver**: Interface web pour visualiser et g√©rer les DAGs
- **Scheduler**: Planifie et d√©clenche les t√¢ches
- **Database**: Stocke l'√©tat des DAGs et des t√¢ches (PostgreSQL)
- **Executor**: Ex√©cute les t√¢ches (LocalExecutor dans notre cas)

### Structure des Fichiers

```
airflow/
‚îú‚îÄ‚îÄ dags/                       # DAGs Airflow
‚îÇ   ‚îú‚îÄ‚îÄ ml_pipeline_dag.py      # Pipeline ML principal
‚îÇ   ‚îî‚îÄ‚îÄ monitoring_dag.py       # Surveillance automatique
‚îú‚îÄ‚îÄ logs/                       # Logs d'ex√©cution
‚îú‚îÄ‚îÄ plugins/                    # Plugins personnalis√©s (si n√©cessaire)
‚îî‚îÄ‚îÄ config/                     # Configuration additionnelle
```

## üöÄ Installation et D√©marrage

### Premi√®re Installation

```bash
# 1. Initialiser Airflow
./scripts/init_airflow.sh

# 2. D√©marrer les services
docker-compose up -d

# 3. V√©rifier l'√©tat
docker-compose ps
```

### Acc√®s √† l'Interface Web

- URL: http://localhost:8080
- Username: `airflow`
- Password: `airflow`

### Commandes Utiles

```bash
# D√©marrer Airflow
docker-compose up -d

# Arr√™ter Airflow
docker-compose down

# Voir les logs
docker-compose logs -f

# Red√©marrer un service
docker-compose restart airflow-scheduler

# Nettoyer tout (‚ö†Ô∏è supprime la BDD)
docker-compose down -v
```

## üìä DAGs Disponibles

### 1. ml_pipeline_face_attributes

**Description**: Pipeline complet de ML pour la classification d'attributs faciaux.

**Planification**: Quotidienne (`@daily` √† minuit)

**√âtapes**:

1. **check_environment**: V√©rifie que DVC et les fichiers n√©cessaires sont pr√©sents
2. **check_raw_data**: Attend que les donn√©es brutes soient disponibles
3. **data_preparation**: 
   - Pull des donn√©es avec DVC
   - Pr√©paration des donn√©es d'entra√Ænement
4. **hyperparameter_optimization**: Optimisation avec Hyperopt
5. **model_training**: 
   - Entra√Ænement du mod√®le
   - Push du mod√®le vers DVC remote
6. **model_evaluation**: 
   - √âvaluation du mod√®le
   - Archivage des m√©triques et plots
7. **notify_success**: Notification de fin

**D√©clenchement manuel**:

```bash
# Via l'interface web
# DAGs ‚Üí ml_pipeline_face_attributes ‚Üí Trigger DAG

# Via CLI
docker-compose exec airflow-scheduler \
  airflow dags trigger ml_pipeline_face_attributes
```

### 2. model_monitoring_and_retraining

**Description**: Surveille les performances et d√©clenche un r√©-entra√Ænement si n√©cessaire.

**Planification**: Hebdomadaire (`@weekly`)

**√âtapes**:

1. **check_data_drift**: D√©tecte le data drift (√† impl√©menter avec Evidently)
2. **check_performance**: Compare la performance actuelle au seuil requis (85%)
3. **trigger_retraining** OU **skip_retraining**: D√©cision bas√©e sur la performance
4. **log_monitoring_results**: Sauvegarde les r√©sultats
5. **send_notification**: Notifie l'√©quipe

**Seuils configurables**:

Dans `monitoring_dag.py`:
```python
ACCURACY_THRESHOLD = 0.85  # 85% de pr√©cision minimale
```

Dans `.env`:
```bash
ACCURACY_THRESHOLD=0.85
```

## üõ†Ô∏è Cr√©ation de DAGs Personnalis√©s

### Structure d'un DAG

```python
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator

default_args = {
    'owner': 'mlops-team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'my_custom_dag',
    default_args=default_args,
    description='Description du DAG',
    schedule_interval='@daily',
    catchup=False,
    tags=['custom', 'ml'],
)

# D√©finir les t√¢ches
task1 = BashOperator(
    task_id='task1',
    bash_command='echo "Hello World"',
    dag=dag,
)

task2 = BashOperator(
    task_id='task2',
    bash_command='echo "Task 2"',
    dag=dag,
)

# D√©finir les d√©pendances
task1 >> task2
```

### Types d'Op√©rateurs Utilis√©s

#### BashOperator

Pour ex√©cuter des commandes shell:

```python
from airflow.operators.bash import BashOperator

run_dvc = BashOperator(
    task_id='run_dvc_repro',
    bash_command='cd /opt/airflow/project && dvc repro',
    dag=dag,
)
```

#### PythonOperator

Pour ex√©cuter du code Python:

```python
from airflow.operators.python import PythonOperator

def my_function(**context):
    print("Ex√©cution de ma fonction")
    return "success"

task = PythonOperator(
    task_id='my_task',
    python_callable=my_function,
    dag=dag,
)
```

#### BranchPythonOperator

Pour des d√©cisions conditionnelles:

```python
from airflow.operators.python import BranchPythonOperator

def decide_branch(**context):
    if condition:
        return 'task_if_true'
    else:
        return 'task_if_false'

branch = BranchPythonOperator(
    task_id='branch_task',
    python_callable=decide_branch,
    dag=dag,
)
```

#### FileSensor

Pour attendre qu'un fichier existe:

```python
from airflow.sensors.filesystem import FileSensor

wait_file = FileSensor(
    task_id='wait_for_file',
    filepath='/path/to/file',
    poke_interval=30,  # V√©rifier toutes les 30 secondes
    timeout=600,       # Timeout apr√®s 10 minutes
    dag=dag,
)
```

### Task Groups

Organiser des t√¢ches li√©es:

```python
from airflow.utils.task_group import TaskGroup

with TaskGroup('data_processing', dag=dag) as group:
    task1 = BashOperator(...)
    task2 = BashOperator(...)
    task1 >> task2
```

## üìÖ Planification

### Expressions Schedule

```python
# Quotidien √† minuit
schedule_interval='@daily'

# Hebdomadaire le dimanche √† minuit
schedule_interval='@weekly'

# Mensuel le 1er √† minuit
schedule_interval='@monthly'

# Toutes les heures
schedule_interval='@hourly'

# Cron custom (tous les jours √† 6h30)
schedule_interval='30 6 * * *'

# Manuel uniquement
schedule_interval=None
```

### Catchup

```python
# Ne pas rattraper les ex√©cutions manqu√©es
catchup=False

# Rattraper toutes les ex√©cutions manqu√©es
catchup=True
```

## üîç Monitoring et Debugging

### Voir les Logs

#### Via l'Interface Web

1. DAGs ‚Üí S√©lectionner le DAG
2. Graph ‚Üí Cliquer sur une t√¢che
3. Logs

#### Via CLI

```bash
# Logs d'une t√¢che sp√©cifique
docker-compose exec airflow-scheduler \
  airflow tasks logs ml_pipeline_face_attributes task_id 2024-01-01

# Logs du scheduler
docker-compose logs -f airflow-scheduler

# Logs du webserver
docker-compose logs -f airflow-webserver
```

### √âtats des T√¢ches

- ‚ö™ **None**: Pas encore planifi√©e
- üü° **Scheduled**: Planifi√©e
- üîµ **Queued**: En file d'attente
- üü¢ **Running**: En cours d'ex√©cution
- ‚úÖ **Success**: R√©ussie
- ‚ùå **Failed**: √âchou√©e
- üî¥ **Upstream Failed**: √âchec d'une d√©pendance
- ‚è≠Ô∏è **Skipped**: Saut√©e

### Commandes de Debug

```bash
# Tester une t√¢che sans l'ex√©cuter
docker-compose exec airflow-scheduler \
  airflow tasks test ml_pipeline_face_attributes check_environment 2024-01-01

# Lister les DAGs
docker-compose exec airflow-scheduler airflow dags list

# Voir l'√©tat d'un DAG
docker-compose exec airflow-scheduler \
  airflow dags state ml_pipeline_face_attributes 2024-01-01

# Marquer une t√¢che comme r√©ussie
docker-compose exec airflow-scheduler \
  airflow tasks clear ml_pipeline_face_attributes -t task_id -s 2024-01-01
```

## ‚öôÔ∏è Configuration Avanc√©e

### Variables Airflow

Stocker des configurations:

```bash
# Via CLI
docker-compose exec airflow-scheduler \
  airflow variables set my_variable my_value

# Via l'interface: Admin ‚Üí Variables
```

Utiliser dans un DAG:

```python
from airflow.models import Variable

my_var = Variable.get("my_variable")
```

### Connexions

Pour se connecter √† des services externes:

```bash
# Via l'interface: Admin ‚Üí Connections
# Type: Amazon Web Services
# Conn Id: aws_default
# Extra: {"region_name": "us-east-1"}
```

### XComs

Partager des donn√©es entre t√¢ches:

```python
# Pousser une valeur
def push_function(**context):
    context['task_instance'].xcom_push(key='my_key', value='my_value')

# Tirer une valeur
def pull_function(**context):
    value = context['task_instance'].xcom_pull(
        task_ids='push_task',
        key='my_key'
    )
```

## üîê S√©curit√©

### Secrets

**NE JAMAIS** hardcoder de secrets dans les DAGs!

```python
# ‚ùå MAUVAIS
AWS_KEY = "AKIAIOSFODNN7EXAMPLE"

# ‚úÖ BON - Variables d'environnement
import os
AWS_KEY = os.getenv('AWS_ACCESS_KEY_ID')

# ‚úÖ BON - Airflow Variables (avec encryption)
from airflow.models import Variable
AWS_KEY = Variable.get("aws_key")

# ‚úÖ BON - Airflow Connections
from airflow.hooks.base import BaseHook
conn = BaseHook.get_connection('aws_default')
```

### Permissions

Configurer les r√¥les dans l'interface:
Security ‚Üí List Roles

## üìß Notifications

### Email

Configurer dans `docker-compose.yml`:

```yaml
environment:
  AIRFLOW__SMTP__SMTP_HOST: smtp.gmail.com
  AIRFLOW__SMTP__SMTP_PORT: 587
  AIRFLOW__SMTP__SMTP_USER: your_email@gmail.com
  AIRFLOW__SMTP__SMTP_PASSWORD: your_password
  AIRFLOW__SMTP__SMTP_MAIL_FROM: your_email@gmail.com
```

Dans le DAG:

```python
default_args = {
    'email': ['team@example.com'],
    'email_on_failure': True,
    'email_on_retry': False,
    'email_on_success': False,
}
```

### Slack

Utiliser le SlackWebhookOperator:

```python
from airflow.providers.slack.operators.slack_webhook import SlackWebhookOperator

notify_slack = SlackWebhookOperator(
    task_id='notify_slack',
    http_conn_id='slack_webhook',
    message='Pipeline termin√©!',
    dag=dag,
)
```

## üîÑ Bonnes Pratiques

### 1. Idempotence

Les t√¢ches doivent pouvoir √™tre r√©ex√©cut√©es:

```python
# ‚úÖ BON - Idempotent
def process_data():
    # Supprimer l'output s'il existe
    if os.path.exists(output_file):
        os.remove(output_file)
    # Cr√©er le nouveau fichier
    create_file(output_file)

# ‚ùå MAUVAIS - Non idempotent
def process_data():
    # Ajoute √† un fichier existant
    with open(output_file, 'a') as f:
        f.write(data)
```

### 2. Task Size

Gardez les t√¢ches petites et focalis√©es:

```python
# ‚úÖ BON - T√¢ches s√©par√©es
extract_task >> transform_task >> load_task

# ‚ùå MAUVAIS - T√¢che monolithique
big_task_that_does_everything
```

### 3. Logging

Loggez abondamment:

```python
import logging

def my_function(**context):
    logging.info("D√©but du traitement")
    result = process_data()
    logging.info(f"Traitement termin√©: {result}")
    return result
```

### 4. Timeouts

Configurez des timeouts:

```python
task = BashOperator(
    task_id='long_task',
    bash_command='long_running_command',
    execution_timeout=timedelta(hours=2),
    dag=dag,
)
```

### 5. Retries

Configurez des retries appropri√©s:

```python
default_args = {
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'retry_exponential_backoff': True,
}
```

## üìù Checklist D√©ploiement

- [ ] Tester le DAG localement: `airflow dags test`
- [ ] V√©rifier la syntaxe Python
- [ ] Documenter le DAG (docstring)
- [ ] Configurer les retries
- [ ] Ajouter du logging
- [ ] Tester l'idempotence
- [ ] Configurer les notifications
- [ ] D√©finir les SLAs si n√©cessaire
- [ ] Tester avec de petits datasets

## üîó Ressources

- [Documentation Airflow](https://airflow.apache.org/docs/)
- [Best Practices](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html)
- [Airflow Concepts](https://airflow.apache.org/docs/apache-airflow/stable/concepts/index.html)

## üí° Tips & Tricks

### 1. D√©veloppement Local

Tester un DAG sans Docker:

```bash
# Installer Airflow localement
pip install apache-airflow

# Tester la syntaxe
python airflow/dags/my_dag.py

# Tester une t√¢che
airflow tasks test my_dag task_id 2024-01-01
```

### 2. Pauser des DAGs

```bash
# Via CLI
airflow dags pause my_dag
airflow dags unpause my_dag
```

### 3. Backfill

Ex√©cuter le DAG sur une p√©riode pass√©e:

```bash
airflow dags backfill my_dag \
  --start-date 2024-01-01 \
  --end-date 2024-01-31
```

### 4. Clear Tasks

R√©initialiser des t√¢ches pour les r√©ex√©cuter:

```bash
airflow tasks clear my_dag \
  --start-date 2024-01-01 \
  --end-date 2024-01-31
```
